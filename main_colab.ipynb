{"cells":[{"cell_type":"code","execution_count":1,"id":"0mdqwxiC7pUP","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":366,"status":"ok","timestamp":1650988620754,"user":{"displayName":"Jihoon Kim","userId":"18310368736161389171"},"user_tz":-60},"id":"0mdqwxiC7pUP","outputId":"d0079593-a815-488f-c20d-134dd6e0082e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Apr 26 15:57:00 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   49C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"id":"b5ce83c2","metadata":{"executionInfo":{"elapsed":2982,"status":"ok","timestamp":1650988625048,"user":{"displayName":"Jihoon Kim","userId":"18310368736161389171"},"user_tz":-60},"id":"b5ce83c2"},"outputs":[],"source":["import os\n","import h5py\n","import numpy as np\n","from tqdm.auto import tqdm\n","import random\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchsummary import summary\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML"]},{"cell_type":"code","execution_count":3,"id":"1cbc53c4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":410,"status":"ok","timestamp":1650988625897,"user":{"displayName":"Jihoon Kim","userId":"18310368736161389171"},"user_tz":-60},"id":"1cbc53c4","outputId":"5b12262c-8a6b-49e5-ec75-ba5b921a3182"},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Seed:  42\n","batch size: 100 mini batch: 50 k: 2\n","dim: 128\n","device: cuda\n"]}],"source":["batch_size = 100\n","mini_batch_size = 50\n","lr_G = 0.0025\n","lr_D = 0.0001\n","beta1 = 0.5\n","workers = 0\n","dataset_name = 'shapenet_v2'\n","obj = 'airplane'\n","dim = 128\n","noise_dim = 200 # latent space vector dim\n","in_channels = 512 # convolutional channels\n","run_parallel = False"]},{"cell_type":"code","execution_count":null,"id":"797ccc65","metadata":{},"outputs":[],"source":["k = int(batch_size / mini_batch_size)\n","print('batch size:', batch_size, 'mini batch:', mini_batch_size, 'k:', k)\n","\n","# Set random seed for reproducibility\n","manualSeed = 42\n","#manualSeed = random.randint(1, 10000) # use if you want new results\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)\n","\n","inverse_scale = 1\n","print('dim:', dim)\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('device:', device)"]},{"cell_type":"code","execution_count":4,"id":"RpyyF4jvYrGu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1977,"status":"ok","timestamp":1650988842660,"user":{"displayName":"Jihoon Kim","userId":"18310368736161389171"},"user_tz":-60},"id":"RpyyF4jvYrGu","outputId":"a9408c33-1669-4173-debd-6e646ba7a7b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"id":"8e1e1e90","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1650988843425,"user":{"displayName":"Jihoon Kim","userId":"18310368736161389171"},"user_tz":-60},"id":"8e1e1e90"},"outputs":[],"source":["project_path = '/content/drive/MyDrive/diss/3D-GAN'\n","data_path = f'{project_path}/data'\n","data_filename = f'{dataset_name}_{obj}_r{dim}'\n","weights_path = f'{project_path}/weights/{data_filename}'\n","src_path = f'{project_path}/3D-GAN'\n","\n","os.makedirs(weights_path, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"id":"stock-reset","metadata":{"id":"stock-reset"},"outputs":[],"source":["f = h5py.File(f'{data_path}/{data_filename}.h5', 'r')\n","dataset = torch.from_numpy(np.array(f[list(f.keys())[0]]).reshape(-1, 1, dim, dim, dim)).to(torch.float)\n","if inverse_scale > 1:\n","    assert type(inverse_scale)==int\n","    dataset = dataset[:, :, ::inverse_scale, ::inverse_scale, ::inverse_scale]\n","\n","print('dataset shape:', dataset.shape)"]},{"cell_type":"code","execution_count":null,"id":"ebdd0b27","metadata":{"id":"ebdd0b27"},"outputs":[],"source":["dataloader = DataLoader(\n","    dataset, \n","    batch_size=batch_size,\n","    shuffle=True, \n","    num_workers=workers,\n",")\n","\n","print('number of batches:', len(dataloader))"]},{"cell_type":"markdown","id":"480cc71e","metadata":{"id":"480cc71e"},"source":["# GAN Structure "]},{"cell_type":"code","execution_count":null,"id":"5fc01451","metadata":{"id":"5fc01451"},"outputs":[],"source":["import sys\n","sys.path.append(src_path) # adding path to sys\n","\n","from src.GAN import Discriminator, Generator, weights_init\n","\n","netG = Generator(in_channels=256, out_dim=dim, out_channels=1, noise_dim=noise_dim)\n","if run_parallel:\n","    netG = torch.nn.DataParallel(netG)\n","netG = netG.to(device)\n","netG.apply(weights_init)\n","# noise = torch.rand(1, noise_dim).to(device)\n","# generated_volume = netG(noise)\n","# print(\"Generator output shape\", generated_volume.shape)\n","netD = Discriminator(in_channels=1, out_conv_channels=256, dim=dim)\n","if run_parallel:\n","    netD = torch.nn.DataParallel(netD)\n","netD = netD.to(device)\n","netD.apply(weights_init)\n","\n","criterion = torch.nn.BCELoss()\n","# # Establish convention for real and fake labels during training\n","# real_label = 1.\n","# fake_label = 0.\n","\n","# Setup Adam optimizers for both G and D\n","optimizerD = optim.Adam(netD.parameters(), lr=lr_D, betas=(beta1, 0.999))\n","optimizerG = optim.Adam(netG.parameters(), lr=lr_G, betas=(beta1, 0.999))\n","\n","# out = netD(generated_volume)\n","# print(\"Discriminator output\", out.item())\n","\n","print(\"\\n\\nGenerator summary\\n\\n\")\n","summary(netG, (1, noise_dim))\n","print(\"\\n\\nDiscriminator summary\\n\\n\")\n","summary(netD, (1, dim, dim, dim))"]},{"cell_type":"markdown","id":"19e0a849","metadata":{"id":"19e0a849"},"source":["# Training and Testing 3D-GAN"]},{"cell_type":"markdown","id":"354d0e98","metadata":{"id":"354d0e98"},"source":["# Running Epochs"]},{"cell_type":"code","execution_count":null,"id":"z3FhtBYujRZE","metadata":{"id":"z3FhtBYujRZE"},"outputs":[],"source":["def plot_convergence(G_losses, D_real_losses, D_fake_losses, real_accuracies, fake_accuracies):\n","  lst_epoch = np.array(range(iters* len(data_split))) / len(dataloader)\n","  plt.figure(figsize=(10,5))\n","  plt.title(\"Generator and Discriminator Loss During Training\")\n","  plt.plot(lst_epoch, G_losses,label=\"G\")\n","  plt.plot(lst_epoch, D_real_losses, label=\"D_real\")\n","  plt.plot(lst_epoch, D_fake_losses, label=\"D_fake\")\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Loss\")\n","  plt.legend()\n","  plt.show()\n","\n","  plt.figure(figsize=(10,5))\n","  plt.title(\"Discriminator Accuracies During Training\")\n","  plt.plot(lst_epoch, real_accuracies, label=\"acc_real\")\n","  plt.plot(lst_epoch, fake_accuracies, label=\"acc_fake\")\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Accuracy\")\n","  plt.legend()\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"id":"c0abfa59","metadata":{"id":"c0abfa59"},"outputs":[],"source":["# Lists to keep track of progress\n","G_losses = []\n","D_real_losses = []\n","D_fake_losses = []\n","real_accuracies = []\n","fake_accuracies = []\n","start_epoch = 0\n","iters = 0\n","\n","real_label = 1.\n","fake_label = 0.\n","\n","os.makedirs(f'{project_path}/weights/{data_filename}', exist_ok=True)"]},{"cell_type":"code","execution_count":null,"id":"Hs1Wom4q5fuH","metadata":{"id":"Hs1Wom4q5fuH"},"outputs":[],"source":["# successful_weights_path = f'{project_path}/weights/successful/{data_filename}'\n","# start_epoch = 490\n","# file_netG = f'{successful_weights_path}/netG_e{start_epoch}_r{dim}_weights.pth'\n","# print(file_netG)\n","# netG.load_state_dict(torch.load(file_netG))\n","# file_netD = f'{successful_weights_path}/netD_e{start_epoch}_r{dim}_weights.pth'\n","# print(file_netD)\n","# netD.load_state_dict(torch.load(file_netD))\n","\n","# print('netG and netD weights set to previous successful ones')"]},{"cell_type":"code","execution_count":null,"id":"267b7539","metadata":{"id":"267b7539","scrolled":true},"outputs":[],"source":["num_epochs = 100\n","\n","# Training Loop\n","print(\"Starting Training Loop...\")\n","# For each epoch\n","for epoch in tqdm(range(start_epoch, start_epoch+num_epochs)):\n","    # For each batch in the dataloader\n","    lst_train_acc_real = []\n","    lst_train_acc_fake = []\n","    for i, data_all in enumerate(dataloader, 0):\n","        data_split = torch.split(data_all, mini_batch_size)\n","        optimizerD.zero_grad()\n","#         print('reset netD grads')\n","        ############################\n","        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n","        ###########################\n","        ## Train with all-real batch\n","        # Format batch\n","        for j in range(len(data_split)):\n","            data = data_split[j]\n","#             print(data.shape)\n","            real_cpu = data.to(device)\n","            b_size = real_cpu.size(0)\n","            label_real = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n","            label_fake = torch.full((b_size,), fake_label, dtype=torch.float, device=device)\n","            # Forward pass real batch through D\n","            output_real = netD(real_cpu).view(-1)\n","            # Calculate loss on all-real batch\n","            D_x = output_real.mean().item()\n","            train_acc_real = torch.sum((output_real > 0.5).to(int)==label_real) / b_size\n","            lst_train_acc_real.append(train_acc_real.item())\n","            errD_real = criterion(output_real, label_real) / len(data_split)\n","            errD_real.backward()\n","\n","            ## Train with all-fake batch\n","            # Generate batch of latent vectors\n","            noise = torch.randn(b_size, noise_dim, device=device)\n","            # Generate fake image batch with G\n","            fake = netG(noise).detach()\n","            output_fake = netD(fake).view(-1)\n","            D_G_z1 = output_fake.mean().item()\n","            train_acc_fake = torch.sum((output_fake > 0.5).to(int) == label_fake) / b_size\n","            lst_train_acc_fake.append(train_acc_fake.item())\n","            errD_fake = criterion(output_fake, label_fake) / len(data_split)\n","            errD_fake.backward()\n","\n","            errD = errD_real + errD_fake        \n","        \n","        # update D only if classification acc is less than 80% for stability\n","#         if (i+1) % k == 0 or (i+1) == len(dataloader):\n","            if j==len(data_split)-1:\n","                acc_real_mean = np.mean(lst_train_acc_real)\n","                acc_fake_mean = np.mean(lst_train_acc_fake)\n","                update = ((acc_real_mean + acc_fake_mean) / 2) < 0.8\n","                if update:\n","                    optimizerD.step()  # update the weights only after accumulating k small batches\n","#                     print('updated optD')\n","\n","                optimizerD.zero_grad()  # reset gradients for accumulation for the next large batch\n","                lst_train_acc_real = []\n","                lst_train_acc_fake = []\n","            \n","        \n","\n","        ############################\n","        # (2) Update G network: maximize log(D(G(z)))\n","        ###########################\n","        # fake labels are real for generator cost\n","        optimizerG.zero_grad()\n","#         print('reset netG grads')\n","        for j in range(len(data_split)):\n","            label = torch.full((b_size,), real_label, dtype=torch.float, device=device)  \n","            # Since we just updated D, perform another forward pass of all-fake batch through D\n","            fake = netG(noise)\n","            output = netD(fake).view(-1)\n","            errG = criterion(output, label) / len(data_split)\n","            errG.backward()\n","\n","\n","            D_G_z2 = output.mean().item()\n","#             if (i+1) % k == 0 or (i+1) == len(dataloader):\n","            if j==len(data_split)-1:\n","                optimizerG.step()  # update the weights only after accumulating k small batches\n","                optimizerG.zero_grad()  # reset gradients for accumulation for the next large_batch\n","#                 print('updated optG')\n","\n","            # Save Losses for plotting later\n","            G_losses.append(errG.item())\n","            D_fake_losses.append(errD_fake.item())\n","            D_real_losses.append(errD_real.item())\n","            fake_accuracies.append(train_acc_fake.item())\n","            real_accuracies.append(train_acc_real.item())\n","\n","        # Output training stats\n","        if i % 10 == 0: # print progress every epoch\n","            print(f'[{epoch}/{start_epoch+num_epochs}][{i}/{len(dataloader)}]\\tLoss_D: {round(errD.item(), 4)}\\tLoss_G: {round(errG.item(), 4)}\\tD(x): {round(D_x, 4)}\\tD(G(z)): {round(D_G_z1, 4)} / {round(D_G_z2, 4)}')\n","            \n","        iters += 1\n","    \n","    if epoch % 10 == 0 and epoch != 0:\n","      plot_convergence(G_losses, D_real_losses, D_fake_losses, real_accuracies, fake_accuracies)\n","      # save network weights\n","      netG_filename = f'{weights_path}/netG_e{epoch}_r{dim}_weights.pth'\n","      netD_filename = f'{weights_path}/netD_e{epoch}_r{dim}_weights.pth'\n","      torch.save(netG.state_dict(), netG_filename)\n","      torch.save(netD.state_dict(), netD_filename)\n","      print('saved network weights', netG_filename)\n","\n","\n","torch.save(netG.state_dict(), f'{weights_path}/netG_e{epoch}_r{dim}_weights.pth')\n","torch.save(netD.state_dict(), f'{weights_path}/netD_e{epoch}_r{dim}_weights.pth')\n","start_epoch = epoch # change start to the current"]},{"cell_type":"markdown","id":"d8a21353","metadata":{"id":"d8a21353"},"source":["# Results "]},{"cell_type":"markdown","id":"174248ee","metadata":{"id":"174248ee"},"source":["## Convergence Graph"]},{"cell_type":"code","execution_count":null,"id":"1b30a82f","metadata":{"id":"1b30a82f","scrolled":false},"outputs":[],"source":["plot_convergence(G_losses, D_real_losses, D_fake_losses, real_accuracies, fake_accuracies)"]},{"cell_type":"markdown","id":"e4ef0b2d","metadata":{"id":"e4ef0b2d"},"source":["# Visualisation"]},{"cell_type":"markdown","id":"8f3b2aac","metadata":{"id":"8f3b2aac"},"source":["## Real Samples"]},{"cell_type":"code","execution_count":null,"id":"31f52947","metadata":{"id":"31f52947"},"outputs":[],"source":["real_sample = next(iter(dataloader))"]},{"cell_type":"code","execution_count":null,"id":"b4b0346f","metadata":{"id":"b4b0346f","scrolled":false},"outputs":[],"source":["for i in range(5):\n","    s = real_sample[i][0]\n","    ax = plt.figure().add_subplot(projection='3d')\n","#     ax = plt.figure(figsize=(10, 10)).add_subplot(projection='3d')\n","    ax.voxels(s)\n","    plt.show()"]},{"cell_type":"markdown","id":"27da0c04","metadata":{"id":"27da0c04"},"source":["# Generate fake samples"]},{"cell_type":"code","execution_count":null,"id":"3be921be","metadata":{"id":"3be921be"},"outputs":[],"source":["fake_samples = []\n","n_fake_samples = 5\n","\n","for i in tqdm(range(10, epoch, 10)):    \n","    try:\n","        file_netG = f'{weights_path}/netG_e{i}_r{dim}_weights.pth'\n","        print(file_netG)\n","        netG.load_state_dict(torch.load(file_netG))\n","\n","        fixed_noise = torch.randn(n_fake_samples, noise_dim, device=device)\n","        with torch.no_grad():\n","            fake = netG(fixed_noise).detach().cpu().numpy()\n","        fake_samples.append(fake)\n","        # print('generated fake samples')\n","        \n","        \n","            \n","    except Exception as e:\n","        print(e)\n","        print('epoch', i, 'failed')\n","\n","fake_samples = np.array(fake_samples)\n","fake_samples = fake_samples >= 0.5\n","print('fake sample shape:', fake_samples.shape)"]},{"cell_type":"code","execution_count":null,"id":"I5Ws-OINEO--","metadata":{"id":"I5Ws-OINEO--"},"outputs":[],"source":["fake_samples_path = f'{data_path}/fake_samples'\n","os.makedirs(fake_samples_path, exist_ok=True)\n","h5_filename = f'{fake_samples_path}/fake_samples_{data_filename}.h5'\n","with h5py.File(h5_filename, \"w\") as f:\n","    dset = f.create_dataset(\"tensor\", data=fake_samples)\n","    print(h5_filename, 'saved')"]},{"cell_type":"code","execution_count":null,"id":"2c2b17cc","metadata":{"id":"2c2b17cc"},"outputs":[],"source":["for i in range(0, fake_samples.shape[0]):\n","  print('epoch', (i+1)*10)\n","  for j in range(fake_samples.shape[1]):\n","    voxel_grid = fake_samples[i][j][0] > 0.5\n","    ax = plt.figure().add_subplot(projection='3d')\n","    ax.voxels(voxel_grid)\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"c3rgCVT1WLL7","metadata":{"id":"c3rgCVT1WLL7"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"main_colab.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}
