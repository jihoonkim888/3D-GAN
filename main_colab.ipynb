{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a115c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce83c2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc53c4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "manualSeed = 42\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "run_parallel = False\n",
    "\n",
    "lr_G = 0.0025 #0.0025 # Learning rate for optimizers\n",
    "lr_D = 0.00005 #0.0001\n",
    "beta1 = 0.5 # Beta1 hyperparam for Adam optimizers\n",
    "# workers = 6 # Number of workers for dataloader\n",
    "# workers = torch.cuda.device_count()*4\n",
    "workers = 0\n",
    "batch_size = 100 # update optimizers every batch size\n",
    "mini_batch_size = 50 # mini batch size during training to fit gpu memory\n",
    "k = int(batch_size / mini_batch_size)\n",
    "print('batch size:', batch_size, 'mini batch:', mini_batch_size, 'k:', k)\n",
    "\n",
    "\n",
    "noise_dim = 200 # latent space vector dim\n",
    "in_channels = 512 # convolutional channels\n",
    "\n",
    "dataset_name = 'modelnet'\n",
    "obj = 'chair'\n",
    "original_dim = 128  # cube volume\n",
    "inverse_scale = 1\n",
    "dim = int(original_dim / inverse_scale)\n",
    "print('dim:', dim)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e1e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = '/content/drive/MyDrive/diss/3D-GAN'\n",
    "data_path = f'{project_path}/data'\n",
    "data_filename = f'{dataset_name}_{obj}_r{original_dim}'\n",
    "weights_path = f'{project_path}/weights/{data_filename}'\n",
    "\n",
    "os.makedirs(weights_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-reset",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = h5py.File(f'{data_path}/{data_filename}.h5', 'r')\n",
    "dataset = torch.from_numpy(np.array(f[list(f.keys())[0]]).reshape(-1, 1, original_dim, original_dim, original_dim)).to(torch.float)\n",
    "if inverse_scale > 1:\n",
    "    assert type(inverse_scale)==int\n",
    "    dataset = dataset[:, :, ::inverse_scale, ::inverse_scale, ::inverse_scale]\n",
    "\n",
    "print('dataset shape:', dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480cc71e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# GAN Structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc01451",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.model import Discriminator, Generator, weights_init\n",
    "\n",
    "netG = Generator(in_channels=256, out_dim=dim, out_channels=1, noise_dim=noise_dim)\n",
    "if run_parallel:\n",
    "    netG = torch.nn.DataParallel(netG)\n",
    "netG = netG.to(device)\n",
    "netG.apply(weights_init)\n",
    "# noise = torch.rand(1, noise_dim).to(device)\n",
    "# generated_volume = netG(noise)\n",
    "# print(\"Generator output shape\", generated_volume.shape)\n",
    "netD = Discriminator(in_channels=1, out_conv_channels=256, dim=dim)\n",
    "if run_parallel:\n",
    "    netD = torch.nn.DataParallel(netD)\n",
    "netD = netD.to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "# # Establish convention for real and fake labels during training\n",
    "# real_label = 1.\n",
    "# fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr_D, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr_G, betas=(beta1, 0.999))\n",
    "\n",
    "# out = netD(generated_volume)\n",
    "# print(\"Discriminator output\", out.item())\n",
    "\n",
    "print(\"\\n\\nGenerator summary\\n\\n\")\n",
    "summary(netG, (1, noise_dim))\n",
    "print(\"\\n\\nDiscriminator summary\\n\\n\")\n",
    "summary(netD, (1, dim, dim, dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0a849",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training and Testing 3D-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd0b27",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    num_workers=workers,\n",
    ")\n",
    "\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d0e98",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Running Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0abfa59",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Lists to keep track of progress\n",
    "G_losses = []\n",
    "D_real_losses = []\n",
    "D_fake_losses = []\n",
    "real_accuracies = []\n",
    "fake_accuracies = []\n",
    "start_epoch = 0\n",
    "iters = 0\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "os.makedirs(f'./weights/{data_filename}', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b7539",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in tqdm(range(start_epoch, start_epoch+num_epochs)):\n",
    "    # For each batch in the dataloader\n",
    "    lst_train_acc_real = []\n",
    "    lst_train_acc_fake = []\n",
    "    for i, data_all in enumerate(dataloader, 0):\n",
    "        data_split = torch.split(data_all, mini_batch_size)\n",
    "        optimizerD.zero_grad()\n",
    "#         print('reset netD grads')\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        # Format batch\n",
    "        for j in range(len(data_split)):\n",
    "            data = data_split[j]\n",
    "#             print(data.shape)\n",
    "            real_cpu = data.to(device)\n",
    "            b_size = real_cpu.size(0)\n",
    "            label_real = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "            label_fake = torch.full((b_size,), fake_label, dtype=torch.float, device=device)\n",
    "            # Forward pass real batch through D\n",
    "            output_real = netD(real_cpu).view(-1)\n",
    "            # Calculate loss on all-real batch\n",
    "            D_x = output_real.mean().item()\n",
    "            train_acc_real = torch.sum((output_real > 0.5).to(int)==label_real) / b_size\n",
    "            lst_train_acc_real.append(train_acc_real.item())\n",
    "            errD_real = criterion(output_real, label_real) / len(data_split)\n",
    "            errD_real.backward()\n",
    "\n",
    "            ## Train with all-fake batch\n",
    "            # Generate batch of latent vectors\n",
    "            noise = torch.rand(b_size, noise_dim, device=device)\n",
    "            # Generate fake image batch with G\n",
    "            fake = netG(noise).detach()\n",
    "            output_fake = netD(fake).view(-1)\n",
    "            D_G_z1 = output_fake.mean().item()\n",
    "            train_acc_fake = torch.sum((output_fake > 0.5).to(int) == label_fake) / b_size\n",
    "            lst_train_acc_fake.append(train_acc_fake.item())\n",
    "            errD_fake = criterion(output_fake, label_fake) / len(data_split)\n",
    "            errD_fake.backward()\n",
    "\n",
    "            errD = errD_real + errD_fake        \n",
    "        \n",
    "        # update D only if classification acc is less than 80% for stability\n",
    "#         if (i+1) % k == 0 or (i+1) == len(dataloader):\n",
    "            if j==len(data_split)-1:\n",
    "                acc_real_mean = np.mean(lst_train_acc_real)\n",
    "                acc_fake_mean = np.mean(lst_train_acc_fake)\n",
    "                update = ((acc_real_mean + acc_fake_mean) / 2) < 0.8\n",
    "                if update:\n",
    "                    optimizerD.step()  # update the weights only after accumulating k small batches\n",
    "#                     print('updated optD')\n",
    "\n",
    "                optimizerD.zero_grad()  # reset gradients for accumulation for the next large batch\n",
    "                lst_train_acc_real = []\n",
    "                lst_train_acc_fake = []\n",
    "            \n",
    "        \n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        # fake labels are real for generator cost\n",
    "        optimizerG.zero_grad()\n",
    "#         print('reset netG grads')\n",
    "        for j in range(len(data_split)):\n",
    "            label = torch.full((b_size,), real_label, dtype=torch.float, device=device)  \n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            fake = netG(noise)\n",
    "            output = netD(fake).view(-1)\n",
    "            errG = criterion(output, label) / len(data_split)\n",
    "            errG.backward()\n",
    "\n",
    "\n",
    "            D_G_z2 = output.mean().item()\n",
    "#             if (i+1) % k == 0 or (i+1) == len(dataloader):\n",
    "            if j==len(data_split)-1:\n",
    "                optimizerG.step()  # update the weights only after accumulating k small batches\n",
    "                optimizerG.zero_grad()  # reset gradients for accumulation for the next large_batch\n",
    "#                 print('updated optG')\n",
    "\n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(errG.item())\n",
    "            D_fake_losses.append(errD_fake.item())\n",
    "            D_real_losses.append(errD_real.item())\n",
    "            fake_accuracies.append(train_acc_fake.item())\n",
    "            real_accuracies.append(train_acc_real.item())\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 10 == 0: # print progress every epoch\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, start_epoch+num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "            \n",
    "        iters += 1\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        # save network weights\n",
    "        netG_filename = f'./weights/{data_filename}/netG_e{epoch}_r{dim}_weights.pth'\n",
    "        netD_filename = f'./weights/{data_filename}/netD_e{epoch}_r{dim}_weights.pth'\n",
    "        torch.save(netG.state_dict(), netG_filename)\n",
    "        torch.save(netD.state_dict(), netD_filename)\n",
    "        print('saved network weights', netG_filename)\n",
    "\n",
    "\n",
    "torch.save(netG.state_dict(), f'./weights/{data_filename}/netG_e{epoch}_r{dim}_weights.pth')\n",
    "torch.save(netD.state_dict(), f'./weights/{data_filename}/netD_e{epoch}_r{dim}_weights.pth')\n",
    "start_epoch = epoch # change start to the current"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a21353",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174248ee",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Convergence Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30a82f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_real_losses, label=\"D_real\")\n",
    "plt.plot(D_fake_losses, label=\"D_fake\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim([0, 5])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Discriminator Accuracies During Training\")\n",
    "plt.plot(real_accuracies, label=\"acc_real\")\n",
    "plt.plot(fake_accuracies, label=\"acc_fake\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef0b2d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b2aac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Real Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f52947",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "real_sample = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0346f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    s = real_sample[i][0]\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "#     ax = plt.figure(figsize=(10, 10)).add_subplot(projection='3d')\n",
    "    ax.voxels(s)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da0c04",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Generate fake samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be921be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fake_samples = []\n",
    "\n",
    "for i in tqdm(range(0, epoch, 20)):    \n",
    "    try:\n",
    "    \n",
    "    #     file_netD = 'weights/shapnet_v2_car_r128/' + 'netD_shapenet_v2_car_r128_e' + f'{i*10}' + '_weights.pth'\n",
    "        file_netG = f'weights/{data_filename}/netG_e{i}_r{dim}_weights.pth'\n",
    "        print(file_netG)\n",
    "        netG.load_state_dict(torch.load(file_netG))\n",
    "    #     netD.load_state_dict(torch.load(file_netD))\n",
    "\n",
    "        fixed_noise = torch.rand(5, noise_dim, device=device)\n",
    "        with torch.no_grad():\n",
    "            fake = netG(fixed_noise).detach().cpu().numpy()\n",
    "        fake_samples.append(fake)\n",
    "        print('generated fake samples')\n",
    "        \n",
    "        \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('epoch', i, 'failed')\n",
    "\n",
    "fake_samples = np.array(fake_samples)\n",
    "\n",
    "os.makedirs('./fake_samples', exist_ok=True)\n",
    "h5_filename = f'./fake_samples/{data_filename}_r{dim}.h5'\n",
    "with h5py.File(h5_filename, \"w\") as f:\n",
    "    dset = f.create_dataset(\"data\", data=fake_samples)\n",
    "    print(h5_filename, 'saved')\n",
    "\n",
    "print('fake sample shape:', fake_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f4754",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b17cc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
